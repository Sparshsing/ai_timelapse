{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sparshsing/ai_timelapse/blob/main/time_lapse_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kgDDtTe46tC",
        "outputId": "c6769ff2-95c1-45dd-adf3-8d0227d21c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ai_timelapse'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 85 (delta 18), reused 78 (delta 13), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (85/85), 14.16 MiB | 9.08 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Sparshsing/ai_timelapse.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kREPNgXA5tnY",
        "outputId": "8ff31145-618d-4541-b443-12d266cd0456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ai_timelapse\n"
          ]
        }
      ],
      "source": [
        "# change directory to ai_timelapse\n",
        "%cd ai_timelapse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lpfHWX8L5EL1",
        "outputId": "2d75c734-07d9-44d0-c427-4c5ca7fb2672"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/ai_timelapse'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ksc3jxlD5guf",
        "outputId": "907805d2-55af-43f1-f1d8-f77ca31fa006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Jan 17 06:22:03 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check GPU (select T4 runtime in settings if error)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "18PAgYHM7cW6",
        "outputId": "a9c300ca-0b2b-4b7b-b18f-be4e0eb4bc45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.9.7)\n",
            "Collecting tensorflow-addons (from -r requirements.txt (line 4))\n",
            "  Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.5.0)\n",
            "Collecting parameterized (from -r requirements.txt (line 7))\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting mediapy (from -r requirements.txt (line 8))\n",
            "  Downloading mediapy-1.2.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.25.0)\n",
            "Collecting apache-beam (from -r requirements.txt (line 10))\n",
            "  Downloading apache_beam-2.61.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (2.27.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (8.4.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (5.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (4.10.0.84)\n",
            "Collecting mediapipe (from -r requirements.txt (line 16))\n",
            "  Downloading mediapipe-0.10.20-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting rembg (from -r requirements.txt (line 17))\n",
            "  Downloading rembg-2.0.61-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (8.1.8)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (0.1.8)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (4.25.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (17.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (1.16.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->-r requirements.txt (line 3)) (0.6.0)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->-r requirements.txt (line 3)) (1.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons->-r requirements.txt (line 4)) (24.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->-r requirements.txt (line 4))\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from mediapy->-r requirements.txt (line 8)) (7.34.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapy->-r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from mediapy->-r requirements.txt (line 8)) (11.1.0)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 9)) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2024.12.12)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 9)) (0.4)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from apache-beam->-r requirements.txt (line 10)) (3.10.14)\n",
            "Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cloudpickle~=2.2.1 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting fasteners<1.0,>=0.3 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading grpcio-1.65.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.11/dist-packages (from apache-beam->-r requirements.txt (line 10)) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam->-r requirements.txt (line 10)) (4.23.0)\n",
            "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading jsonpickle-3.4.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading objsize-0.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading pymongo-4.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from apache-beam->-r requirements.txt (line 10)) (1.25.0)\n",
            "Collecting pydot<2,>=1.2.0 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam->-r requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.11/dist-packages (from apache-beam->-r requirements.txt (line 10)) (2024.2)\n",
            "Collecting redis<6,>=5.0.0 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.11/dist-packages (from apache-beam->-r requirements.txt (line 10)) (2024.11.6)\n",
            "Collecting sortedcontainers>=2.4.0 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam->-r requirements.txt (line 10)) (4.12.2)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=3.12 in /usr/local/lib/python3.11/dist-packages (from apache-beam->-r requirements.txt (line 10)) (6.0.2)\n",
            "Collecting pyarrow (from tensorflow-datasets->-r requirements.txt (line 3))\n",
            "  Downloading pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting pyarrow-hotfix<1 (from apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage->-r requirements.txt (line 11)) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery-storage->-r requirements.txt (line 11)) (2.27.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown->-r requirements.txt (line 13)) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown->-r requirements.txt (line 13)) (3.16.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 16)) (24.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 16)) (24.12.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 16)) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 16)) (0.4.33)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 16)) (4.10.0.84)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe->-r requirements.txt (line 16))\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 16)) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from rembg->-r requirements.txt (line 17)) (4.10.0.84)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.11/dist-packages (from rembg->-r requirements.txt (line 17)) (1.8.2)\n",
            "Collecting pymatting (from rembg->-r requirements.txt (line 17))\n",
            "  Downloading PyMatting-1.1.13-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->-r requirements.txt (line 3)) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->-r requirements.txt (line 3)) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->-r requirements.txt (line 3)) (3.21.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage->-r requirements.txt (line 11)) (1.66.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage->-r requirements.txt (line 11)) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery-storage->-r requirements.txt (line 11)) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery-storage->-r requirements.txt (line 11)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery-storage->-r requirements.txt (line 11)) (4.9)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->-r requirements.txt (line 10)) (1.17.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam->-r requirements.txt (line 10)) (3.2.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->-r requirements.txt (line 10)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->-r requirements.txt (line 10)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->-r requirements.txt (line 10)) (0.22.3)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam->-r requirements.txt (line 10))\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r requirements.txt (line 3)) (2024.12.14)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe->-r requirements.txt (line 16)) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 13)) (2.6)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->-r requirements.txt (line 8)) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->mediapy->-r requirements.txt (line 8))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->-r requirements.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->-r requirements.txt (line 8)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->-r requirements.txt (line 8)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->-r requirements.txt (line 8)) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->-r requirements.txt (line 8)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->-r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->-r requirements.txt (line 8)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->-r requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe->-r requirements.txt (line 16)) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe->-r requirements.txt (line 16)) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy->-r requirements.txt (line 8)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy->-r requirements.txt (line 8)) (1.4.8)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch->rembg->-r requirements.txt (line 17)) (4.3.6)\n",
            "Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.11/dist-packages (from pymatting->rembg->-r requirements.txt (line 17)) (0.60.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 13)) (1.7.1)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple-parsing->tensorflow-datasets->-r requirements.txt (line 3)) (0.16)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe->-r requirements.txt (line 16)) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->mediapy->-r requirements.txt (line 8)) (0.8.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba!=0.49.0->pymatting->rembg->-r requirements.txt (line 17)) (0.43.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->mediapy->-r requirements.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mediapy->-r requirements.txt (line 8)) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery-storage->-r requirements.txt (line 11)) (0.6.1)\n",
            "Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading mediapy-1.2.2-py3-none-any.whl (26 kB)\n",
            "Downloading apache_beam-2.61.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mediapipe-0.10.20-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rembg-2.0.61-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m700.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading grpcio-1.65.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpickle-3.4.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading objsize-0.7.0-py3-none-any.whl (11 kB)\n",
            "Downloading pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "Downloading pymongo-4.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading redis-5.2.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMatting-1.1.13-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: crcmod, dill, hdfs, docopt\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp311-cp311-linux_x86_64.whl size=31656 sha256=d506eb5bd7af5e698da3d9ea3d6ad124505982e198e41e0b01c6183a81b1cb0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/94/7a/8cb7d14597e6395ce969933f01aed9ea8fa5f5b4d4c8a61e99\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=5b375ae641f50251a0b5a87aaa351941ee2ed2a2f82ca43648c0675f630a5146\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/60/80/1622338bcecce31a5664ef01c203cc5a7b09f59588d9c07376\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34324 sha256=994a44fbfa6c76124387e305a6f6de9e0658f028cccb41c0d6aedc2b35e50a5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/1d/dc/eb0833be25464c359903d356c4204721c6a672c26ff164cdc3\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=2faca7b7a1c2f4b758a290416b8bd671a175a8cc4debf0b315681e2fa56f7d0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built crcmod dill hdfs docopt\n",
            "Installing collected packages: sortedcontainers, docopt, crcmod, zstandard, typeguard, redis, pydot, pyarrow-hotfix, pyarrow, parameterized, objsize, jsonpickle, jedi, grpcio, fasteners, fastavro, dnspython, dill, cloudpickle, tensorflow-addons, sounddevice, pymongo, pymatting, hdfs, mediapy, rembg, mediapipe, apache-beam\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.1\n",
            "    Uninstalling typeguard-4.4.1:\n",
            "      Successfully uninstalled typeguard-4.4.1\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 3.0.4\n",
            "    Uninstalling pydot-3.0.4:\n",
            "      Successfully uninstalled pydot-3.0.4\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 17.0.0\n",
            "    Uninstalling pyarrow-17.0.0:\n",
            "      Successfully uninstalled pyarrow-17.0.0\n",
            "  Attempting uninstall: jsonpickle\n",
            "    Found existing installation: jsonpickle 4.0.1\n",
            "    Uninstalling jsonpickle-4.0.1:\n",
            "      Successfully uninstalled jsonpickle-4.0.1\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.69.0\n",
            "    Uninstalling grpcio-1.69.0:\n",
            "      Successfully uninstalled grpcio-1.69.0\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.0\n",
            "    Uninstalling cloudpickle-3.1.0:\n",
            "      Successfully uninstalled cloudpickle-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\n",
            "dask 2024.10.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed apache-beam-2.61.0 cloudpickle-2.2.1 crcmod-1.7 dill-0.3.1.1 dnspython-2.7.0 docopt-0.6.2 fastavro-1.10.0 fasteners-0.19 grpcio-1.65.5 hdfs-2.7.3 jedi-0.19.2 jsonpickle-3.4.2 mediapipe-0.10.20 mediapy-1.2.2 objsize-0.7.0 parameterized-0.9.0 pyarrow-16.1.0 pyarrow-hotfix-0.6 pydot-1.4.2 pymatting-1.1.13 pymongo-4.10.1 redis-5.2.1 rembg-2.0.61 sortedcontainers-2.4.0 sounddevice-0.5.1 tensorflow-addons-0.23.0 typeguard-2.13.3 zstandard-0.23.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UEomJnQh9k5o",
        "outputId": "44e321e8-020e-4759-fd5a-2a6f0e76d0f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.12.23)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.20.1\n"
          ]
        }
      ],
      "source": [
        "!pip install onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61-YrGoj3k2i"
      },
      "source": [
        "## Generate smooth Time lapse from multiple sequencial images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6iPjfJwa3k2j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import tensorflow as tf\n",
        "import mediapy\n",
        "import cv2\n",
        "from PIL import Image, ExifTags\n",
        "\n",
        "from natsort import natsorted\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "import math\n",
        "import mediapy as media\n",
        "import sys\n",
        "from typing import Generator, Iterable, List, Optional\n",
        "\n",
        "\n",
        "from eval import interpolator, util\n",
        "import fix_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snJNybSe3k2j"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sDVZ3dQR3k2k",
        "outputId": "4deec4f3-e72d-4778-e7ca-da4104c8a71e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving folder 1NSex_z_Mm13nobMRDyS6JeXLPhW3bREz assets\n",
            "Retrieving folder 1s9pbFx_bSbinhx5PChJwZqPsyRIlehmZ variables\n",
            "Processing file 1_oyM-LBAK9o7-bNWf1jG8VvBYeqpmSUr variables.data-00000-of-00001\n",
            "Processing file 1ceC2kbJs3U1dMMrp4hNIpoHRFxO33SFC variables.index\n",
            "Processing file 1dT85Z-HyYsiUgIQbOgYFjwWPOw8en1RC keras_metadata.pb\n",
            "Processing file 1nfi15im3LQvCx84ZRiNcfMuodDkRL_Ei saved_model.pb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_oyM-LBAK9o7-bNWf1jG8VvBYeqpmSUr\n",
            "From (redirected): https://drive.google.com/uc?id=1_oyM-LBAK9o7-bNWf1jG8VvBYeqpmSUr&confirm=t&uuid=6b3d9d53-a046-4c86-860f-c62aa0874661\n",
            "To: /content/ai_timelapse/pretrained_models/film_net/Style/saved_model/variables/variables.data-00000-of-00001\n",
            "100%|██████████| 138M/138M [00:02<00:00, 50.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ceC2kbJs3U1dMMrp4hNIpoHRFxO33SFC\n",
            "To: /content/ai_timelapse/pretrained_models/film_net/Style/saved_model/variables/variables.index\n",
            "100%|██████████| 1.93k/1.93k [00:00<00:00, 6.95MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dT85Z-HyYsiUgIQbOgYFjwWPOw8en1RC\n",
            "To: /content/ai_timelapse/pretrained_models/film_net/Style/saved_model/keras_metadata.pb\n",
            "100%|██████████| 222k/222k [00:00<00:00, 75.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nfi15im3LQvCx84ZRiNcfMuodDkRL_Ei\n",
            "To: /content/ai_timelapse/pretrained_models/film_net/Style/saved_model/saved_model.pb\n",
            "100%|██████████| 2.64M/2.64M [00:00<00:00, 112MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "# Download the pretrained model\n",
        "import gdown\n",
        "os.makedirs('pretrained_models/film_net/Style/saved_model', exist_ok=True)\n",
        "\n",
        "if os.path.exists('pretrained_models/film_net/Style/saved_model/saved_model.pb'):\n",
        "    print('Model already downloaded')\n",
        "else:\n",
        "  folder_url = 'https://drive.google.com/drive/folders/1i9Go1YI2qiFWeT5QtywNFmYAA74bhXWj'\n",
        "  gdown.download_folder(folder_url, output='pretrained_models/film_net/Style/saved_model', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nwB5mYvR3k2k"
      },
      "outputs": [],
      "source": [
        "\n",
        "MAX_DIM = 1080\n",
        "# set no of interpolated frames between images\n",
        "MAX_INTERPOLATED_FRAMES = 63  # 1 less than powers of 2\n",
        "MODEL_PATH = \"pretrained_models/film_net/Style/saved_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uVxhyExg3k2k"
      },
      "outputs": [],
      "source": [
        "if MAX_INTERPOLATED_FRAMES + 1 not in (2, 4, 8, 16, 32, 64, 128):\n",
        "    raise ValueError(\"MAX_INTERPOLATED_FRAMES + 1 must be a power of 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vdgoCPVo3k2k"
      },
      "outputs": [],
      "source": [
        "input_dir = 'sample_data/shahrukh-khan'\n",
        "output_dir = 'output_frames/shahrukh-khan'\n",
        "preprocess_faces = True\n",
        "equal_intervals = True  # whether to use equal no of frames between images or decide based on date\n",
        "\n",
        "# applicable when equal_intervals is False\n",
        "# desired gap between frames in seconds (difference between dates of frames)\n",
        "# eg. set 3600 to generate 1 frame for every 1 hr gap in input images\n",
        "min_frame_duration = 3600 * 24\n",
        "\n",
        "processed_input_dir = tempfile.mkdtemp()\n",
        "temp_interpolated_frames_dir = tempfile.mkdtemp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgpvI8G83k2k"
      },
      "source": [
        "## Plan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dVhGZ5ct3k2k"
      },
      "outputs": [],
      "source": [
        "# 1. Preprocess images\n",
        "# 2. for succesive pairs of images:\n",
        "# 3.     generate interpolated frames\n",
        "# 4.     add interpolated frames to the output directory\n",
        "# 5. generate video from output directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WIJ6zNr3k2k"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "peMfh7q33k2k"
      },
      "outputs": [],
      "source": [
        "\n",
        "# create empty output directory\n",
        "def create_empty_dir(dir_name):\n",
        "  if os.path.exists(dir_name):\n",
        "    shutil.rmtree(dir_name)\n",
        "  os.makedirs(dir_name)\n",
        "\n",
        "\n",
        "def resize_and_save(input_path, output_path, size):\n",
        "  if input_path.endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
        "      img = Image.open(input_path)\n",
        "      img = img.resize(size, Image.Resampling.LANCZOS)\n",
        "      img.save(output_path)\n",
        "  else:\n",
        "      raise ValueError(f'Unsupported file format: {input_path}')\n",
        "\n",
        "\n",
        "# prepare input data\n",
        "def prepare_images(input_dir, processed_input_dir, preprocess_faces, size):\n",
        "  files = natsorted([f for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
        "  if len(files) < 0:\n",
        "    raise FileNotFoundError('no images found in input directory')\n",
        "\n",
        "  for f in tqdm(files, desc='Preparing images'):\n",
        "    if preprocess_faces:\n",
        "       fix_images.process_face_image(\n",
        "            os.path.join(input_dir, f),\n",
        "            os.path.join(processed_input_dir, f),\n",
        "            background_color=(255, 255, 255),  # White background\n",
        "            save_bbox_preview=False\n",
        "            )\n",
        "    else:\n",
        "        resize_and_save(os.path.join(input_dir, f), os.path.join(processed_input_dir, f), size)\n",
        "\n",
        "\n",
        "def get_new_size(image_path: str, max_dim: int = MAX_DIM) -> np.ndarray:\n",
        "    \"\"\"Resize the image so that the maximum dimension is `max_dim`.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    h, w = image.shape[:2]\n",
        "    if h > w:\n",
        "        new_h = max_dim\n",
        "        new_w = int(w * new_h / h)\n",
        "    else:\n",
        "        new_w = max_dim\n",
        "        new_h = int(h * new_w / w)\n",
        "    return (new_w, new_h)\n",
        "\n",
        "\n",
        "def string_to_timestamp(datetime_str):\n",
        "    # Convert the datetime string to a datetime object\n",
        "    timestamp = datetime.strptime(datetime_str, \"%Y%m%d_%H%M%S\")\n",
        "    return timestamp\n",
        "\n",
        "\n",
        "def timestamp_to_string(timestamp):\n",
        "    # Convert the timestamp number to a datetime object\n",
        "    dt = datetime.fromtimestamp(timestamp)\n",
        "    # Format the datetime object to the desired string format\n",
        "    formatted_string = dt.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    return formatted_string\n",
        "\n",
        "\n",
        "def choose_evenly_spaced_timestamps(timestamps, k):\n",
        "    # Ensure we have at least k timestamps\n",
        "    if k > len(timestamps):\n",
        "        raise ValueError(\"k cannot be greater than the number of timestamps\")\n",
        "\n",
        "    # Calculate the interval between timestamps\n",
        "    n = len(timestamps)\n",
        "    interval = (n - 1) / (k - 1)\n",
        "\n",
        "    # Select the timestamps\n",
        "    chosen_timestamps = []\n",
        "    for i in range(k):\n",
        "        index = round(i * interval)\n",
        "        chosen_timestamps.append(timestamps[index])\n",
        "\n",
        "    return chosen_timestamps\n",
        "\n",
        "\n",
        "def save_video(frames, out_path):\n",
        "    ffmpeg_path = util.get_ffmpeg_path()\n",
        "    mediapy.set_ffmpeg(ffmpeg_path)\n",
        "    mediapy.write_video(out_path, frames, fps=30)\n",
        "\n",
        "\n",
        "def save_frames(frames, output_dir, format='jpg'):\n",
        "    \"\"\"\n",
        "    Save interpolated frames to the specified output directory and return the output paths.\n",
        "    Args:\n",
        "        frames: List of image arrays\n",
        "        output_dir: Directory to save frames\n",
        "        format: Image format to save (jpg/png)\n",
        "    Returns:\n",
        "        list: List of file paths where the frames are saved.\n",
        "    \"\"\"\n",
        "    output_paths = []\n",
        "    for idx, frame in enumerate(frames):\n",
        "        output_path = os.path.join(output_dir, f'frame_{idx:06d}.{format}')\n",
        "        util.write_image(output_path, frame)\n",
        "        output_paths.append(output_path)\n",
        "    return output_paths\n",
        "\n",
        "\n",
        "def write_video_from_images(image_dir, output_path, fps):\n",
        "    image_files = natsorted([f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
        "\n",
        "    if not image_files:\n",
        "        print(\"No images found in the directory.\")\n",
        "        return\n",
        "\n",
        "    # Read the first image to get the dimensions\n",
        "    first_image_path = os.path.join(image_dir, image_files[0])\n",
        "    first_frame = cv2.imread(first_image_path)\n",
        "    height, width, layers = first_frame.shape\n",
        "\n",
        "    # Initialize the video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Specify the codec\n",
        "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        frame = cv2.imread(image_path)\n",
        "        if frame is None:\n",
        "            print(f\"Skipping {image_path}, cannot read image.\")\n",
        "            continue\n",
        "        video_writer.write(frame)\n",
        "\n",
        "    video_writer.release()\n",
        "    print(f\"Video saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3xUYjgHd3k2k"
      },
      "outputs": [],
      "source": [
        "\n",
        "def interpolate_frames(image_1, image_2, times_to_interpolate, interpolator):\n",
        "  input_frames = [str(image_1), str(image_2)]\n",
        "\n",
        "  frames = list(\n",
        "      util.interpolate_recursively_from_files(\n",
        "          input_frames, times_to_interpolate, interpolator))\n",
        "  return frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNI760nM3k2l"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzC7E2oK3k2l",
        "outputId": "fd84bc90-10e5-44e1-be08-30aebb1d4247"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rPreparing images:   0%|          | 0/13 [00:00<?, ?it/s]Downloading data from 'https://github.com/danielgatis/rembg/releases/download/v0.0.0/u2net.onnx' to file '/root/.u2net/u2net.onnx'.\n",
            "\n",
            "  0%|                                               | 0.00/176M [00:00<?, ?B/s]\u001b[A\n",
            "  3%|█▏                                    | 5.35M/176M [00:00<00:03, 53.5MB/s]\u001b[A\n",
            "  6%|██▎                                   | 10.7M/176M [00:00<00:03, 52.2MB/s]\u001b[A\n",
            "  9%|███▍                                  | 16.0M/176M [00:00<00:03, 52.6MB/s]\u001b[A\n",
            " 12%|████▌                                 | 21.3M/176M [00:00<00:02, 52.4MB/s]\u001b[A\n",
            " 15%|█████▊                                | 26.9M/176M [00:00<00:02, 53.6MB/s]\u001b[A\n",
            " 18%|███████                               | 32.5M/176M [00:00<00:02, 54.5MB/s]\u001b[A\n",
            " 22%|████████▏                             | 38.0M/176M [00:00<00:02, 51.8MB/s]\u001b[A\n",
            " 25%|█████████▎                            | 43.2M/176M [00:00<00:02, 50.6MB/s]\u001b[A\n",
            " 27%|██████████▍                           | 48.2M/176M [00:00<00:02, 50.5MB/s]\u001b[A\n",
            " 30%|███████████▌                          | 53.5M/176M [00:01<00:02, 51.0MB/s]\u001b[A\n",
            " 33%|████████████▋                         | 58.7M/176M [00:01<00:02, 51.5MB/s]\u001b[A\n",
            " 36%|█████████████▊                        | 63.9M/176M [00:01<00:02, 49.6MB/s]\u001b[A\n",
            " 39%|██████████████▊                       | 68.9M/176M [00:01<00:02, 48.1MB/s]\u001b[A\n",
            " 42%|███████████████▉                      | 73.7M/176M [00:01<00:02, 45.7MB/s]\u001b[A\n",
            " 44%|████████████████▉                     | 78.3M/176M [00:01<00:02, 45.2MB/s]\u001b[A\n",
            " 47%|█████████████████▉                    | 82.9M/176M [00:01<00:02, 45.4MB/s]\u001b[A\n",
            " 50%|██████████████████▉                   | 87.7M/176M [00:01<00:01, 46.3MB/s]\u001b[A\n",
            " 53%|████████████████████                  | 92.8M/176M [00:01<00:01, 47.5MB/s]\u001b[A\n",
            " 56%|█████████████████████                 | 97.8M/176M [00:01<00:01, 48.4MB/s]\u001b[A\n",
            " 58%|██████████████████████▊                | 103M/176M [00:02<00:01, 49.1MB/s]\u001b[A\n",
            " 61%|███████████████████████▉               | 108M/176M [00:02<00:01, 48.8MB/s]\u001b[A\n",
            " 64%|████████████████████████▉              | 113M/176M [00:02<00:01, 47.3MB/s]\u001b[A\n",
            " 67%|██████████████████████████             | 117M/176M [00:02<00:01, 41.1MB/s]\u001b[A\n",
            " 69%|██████████████████████████▉            | 122M/176M [00:02<00:01, 39.2MB/s]\u001b[A\n",
            " 72%|███████████████████████████▉           | 126M/176M [00:02<00:01, 40.1MB/s]\u001b[A\n",
            " 75%|█████████████████████████████▎         | 132M/176M [00:02<00:00, 46.2MB/s]\u001b[A\n",
            " 79%|██████████████████████████████▋        | 139M/176M [00:02<00:00, 51.2MB/s]\u001b[A\n",
            " 83%|████████████████████████████████▎      | 146M/176M [00:02<00:00, 58.0MB/s]\u001b[A\n",
            " 87%|█████████████████████████████████▉     | 153M/176M [00:03<00:00, 62.2MB/s]\u001b[A\n",
            " 91%|███████████████████████████████████▌   | 161M/176M [00:03<00:00, 65.2MB/s]\u001b[A\n",
            " 95%|█████████████████████████████████████▏ | 168M/176M [00:03<00:00, 67.3MB/s]\u001b[A\n",
            " 99%|██████████████████████████████████████▋| 175M/176M [00:03<00:00, 65.2MB/s]\u001b[A\n",
            "100%|███████████████████████████████████████| 176M/176M [00:00<00:00, 31.5GB/s]\n",
            "Preparing images: 100%|██████████| 13/13 [00:34<00:00,  2.65s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preprocess images\n",
        "files = natsorted([f for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
        "new_size = get_new_size(os.path.join(input_dir, files[0]))\n",
        "create_empty_dir(processed_input_dir)\n",
        "prepare_images(input_dir, processed_input_dir, preprocess_faces, new_size)\n",
        "files = natsorted([f for f in os.listdir(processed_input_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
        "len(files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5gnR-pS3k2l",
        "outputId": "5a23d5eb-8847-447d-9bda-ce95caa4d138"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: 1992_deewana.jpg\n",
            "Filename: 1993_baazigar.jpg\n",
            "Filename: 1995_ddlj.jpg\n",
            "Filename: 1998_kuch_kuch_hota_hai.jpg\n",
            "Filename: 2001_kbkg.jpg\n",
            "Filename: 2003_kal_ho_na_ho.jpeg\n",
            "Filename: 2006_kabh_alvida.jpg\n",
            "Filename: 2009_billu.jpg\n",
            "Filename: 2011_don.jpg\n",
            "Filename: 2014_hny.jpg\n",
            "Filename: 2017_raees.jpg\n",
            "Filename: 2019_media.jpg\n",
            "Filename: 2023_pathan.jpg\n"
          ]
        }
      ],
      "source": [
        "if not equal_intervals:\n",
        "    for file in files:\n",
        "      timestamp = string_to_timestamp(file.split('.')[0])\n",
        "      print(f\"Filename: {file}, Datetime: {timestamp}\")\n",
        "else:\n",
        "   for file in files:\n",
        "      print(f\"Filename: {file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "344GgQzX3k2l",
        "outputId": "b1736c9e-089b-4013-8a28-8a55e63e1e13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total frames: 13\n",
            "processing frame 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m███████████████████████████████████████████████████████████████\u001b[0m| 63/63 [01:49<00:00,  1.74s/it]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing frame 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m███████████████████████████████████████████████████████████████\u001b[0m| 63/63 [01:29<00:00,  1.42s/it]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing frame 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m███████████████████████████████████████████████████████████████\u001b[0m| 63/63 [01:28<00:00,  1.41s/it]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing frame 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m███████████████████████████████████████████████████████████████\u001b[0m| 63/63 [01:28<00:00,  1.41s/it]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing frame 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m███████████████████████████████████████████████████████████████\u001b[0m| 63/63 [01:28<00:00,  1.41s/it]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing frame 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m███████████████████████████████████████████████████████████████\u001b[0m| 63/63 [01:28<00:00,  1.41s/it]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing frame 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m███████████████████████████████████████████████████████████████\u001b[0m| 63/63 [01:28<00:00,  1.41s/it]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing frame 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m███████████████████████████████████████████████████████████████\u001b[0m| 63/63 [01:28<00:00,  1.41s/it]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing frame 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m███████████████████████████████████████████████████████████████\u001b[0m| 63/63 [01:28<00:00,  1.41s/it]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing frame 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m███████████████████████████████████████████████████████████████\u001b[0m| 63/63 [01:28<00:00,  1.41s/it]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing frame 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m███████████████████████████████████████████████████████████████\u001b[0m| 63/63 [01:28<00:00,  1.41s/it]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing frame 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m███████████████████████████████████████████████████████████████\u001b[0m| 63/63 [01:28<00:00,  1.41s/it]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Iterate through pairs of consecutive frames\n",
        "\n",
        "interpolator_model = interpolator.Interpolator(MODEL_PATH, None)\n",
        "print('Total frames:', len(files))\n",
        "create_empty_dir(output_dir)\n",
        "for i in range(len(files) - 1):\n",
        "    print('processing frame', i)\n",
        "    start_filename = files[i]\n",
        "    end_filename = files[i + 1]\n",
        "\n",
        "    if not equal_intervals:\n",
        "        start_time = string_to_timestamp(start_filename.split('.')[0])\n",
        "        end_time = string_to_timestamp(end_filename.split('.')[0])\n",
        "\n",
        "    # Calculate times_to_interpolate for the required number of intermediate frames\n",
        "    if equal_intervals:\n",
        "          num_frames_needed = MAX_INTERPOLATED_FRAMES\n",
        "    else:\n",
        "          # Calculate the time difference in hours\n",
        "          time_diff = (end_time - start_time).total_seconds()\n",
        "          num_frames_needed = round(time_diff / min_frame_duration) - 1\n",
        "          num_frames_needed = min(num_frames_needed, MAX_INTERPOLATED_FRAMES)\n",
        "    times_to_interpolate = math.ceil(math.log2(num_frames_needed+1))  # n_intermediate = 2^k - 1\n",
        "\n",
        "    frame_1 = os.path.join(processed_input_dir, start_filename)\n",
        "    frame_2 = os.path.join(processed_input_dir, end_filename)\n",
        "    output_frames = interpolate_frames(frame_1, frame_2, times_to_interpolate, interpolator_model)\n",
        "\n",
        "    create_empty_dir(temp_interpolated_frames_dir)\n",
        "    save_frames(output_frames, temp_interpolated_frames_dir)\n",
        "    interpolated_files = natsorted([f for f in os.listdir(temp_interpolated_frames_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
        "\n",
        "    if not equal_intervals:\n",
        "        timestamps = np.linspace(start_time.timestamp(), end_time.timestamp(), len(interpolated_files))\n",
        "        # find required frames from generated frames (can be extra)\n",
        "        chosen_timestamps = choose_evenly_spaced_timestamps(timestamps, num_frames_needed+2)\n",
        "\n",
        "        for filename, timestamp in zip(interpolated_files, timestamps):\n",
        "            if timestamp not in chosen_timestamps:\n",
        "                continue\n",
        "            new_filename = f\"{timestamp_to_string(timestamp)}{os.path.splitext(filename)[1]}\"\n",
        "            shutil.copyfile(os.path.join(temp_interpolated_frames_dir, filename), os.path.join(output_dir, new_filename))\n",
        "    else:\n",
        "        for interp_index, filename in enumerate(interpolated_files[:-1]):\n",
        "          new_filename = f\"{start_filename.split('.')[0]}_{interp_index}{os.path.splitext(filename)[1]}\"\n",
        "          shutil.copyfile(os.path.join(temp_interpolated_frames_dir, filename), os.path.join(output_dir, new_filename))\n",
        "          # include end frame in last iteration\n",
        "          if i == len(files) - 2:\n",
        "              new_filename = f\"{end_filename.split('.')[0]}_0{os.path.splitext(filename)[1]}\"\n",
        "              shutil.copyfile(os.path.join(processed_input_dir, end_filename), os.path.join(output_dir, new_filename))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC0lnT4o3k2l",
        "outputId": "34490167-2e78-4966-b600-2d3cdcce9be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video saved to: output_frames/shahrukh-khan.mp4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# generate video from output directory\n",
        "output_path = str(output_dir) + '.mp4'\n",
        "fps = 24\n",
        "\n",
        "write_video_from_images(output_dir, output_path, fps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTnaWkjeEH3d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
